{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F # All functions that don't have any parameters\n",
    "import numpy as np\n",
    "from  boxScore  import boxScore\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import DataLoader # data management \n",
    "# \n",
    "from NeuralNetwork import NN\n",
    "from DatasetPrivate import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import winsound\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=\"2019-20\"\n",
    "# stats=\"traditional\"\n",
    "stats='advance'\n",
    "box_score=boxScore(years,stats)\n",
    "\n",
    "x_train, x_test, y_train, y_test=box_score.separation()\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "input_size=len(x_train.columns)\n",
    "activation=['relu','sigmoid']\n",
    "number_neurons=[10,30,50,100,150,200]\n",
    "# number_neurons=[10,30]\n",
    "possible_learning_rate=[0.0001,0.001,0.01]\n",
    "# possible_learning_rate=[0.0001]\n",
    "num_epochs=500\n",
    "batch_size=150\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_LL=DataLoader(MyDataset(x_train,y_train),batch_size=batch_size,shuffle=False)\n",
    "test_LL=DataLoader(MyDataset(x_test,y_test),batch_size=batch_size,shuffle=False)\n",
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config_array=[]\n",
    "best_history_array=[]\n",
    "for num in number_neurons:\n",
    "    best_config_tmp={'val_acc':0}\n",
    "    best_history_tmp=[]\n",
    "    for el in possible_learning_rate:\n",
    "        for act in activation:\n",
    "                \n",
    "                # Initialize model\n",
    "                model = NN(input_size=input_size,size_hidden_level=num,act=act,learning_rate=el).to(device)\n",
    "\n",
    "                \n",
    "                \n",
    "                config={\n",
    "                    \"learning_rate\":el,\n",
    "                    'num_neurons':num,\n",
    "                    'acti_fun':act,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"epochs\": num_epochs,\n",
    "                    \"architecture\": \"OneHiddenLayer\",\n",
    "                    \"dataset\":years+\"_\"+stats\n",
    "                }\n",
    "                \n",
    "                best_config_tmp,best_history_tmp=model.MyTrain(kf=kf,config=config,best_config_tmp=best_config_tmp,\n",
    "                best_history_tmp=best_history_tmp,train_LL=train_LL,test_LL=test_LL,device=device)\n",
    "               \n",
    "                \n",
    "                    \n",
    "    print(\"Best model\",best_config_tmp)\n",
    "    best_config_array.append(best_config_tmp)\n",
    "    best_history_array.append(best_history_tmp)\n",
    "    \n",
    "# Doing the mean and the variance between the 10 fold\n",
    "# (for the variance just the last element of each fold)\n",
    "# (mean for all the vectors of folds) \n",
    "num=0\n",
    "for array,config in  zip(best_history_array,best_config_array):\n",
    "    tmp1=[ torch.mean(el,axis=0) for el in array]\n",
    "    tmp2=[ torch.var(el[:,-1],axis=0) for el in array]\n",
    "    best_history_array[num]=tmp1\n",
    "    config[\"variance_loss\"]=tmp2[0]\n",
    "    config[\"variance_acc\"]=tmp2[1]\n",
    "    config[\"variance_val_loss\"]=tmp2[2]\n",
    "    config[\"variance_val_acc\"]=tmp2[3]\n",
    "    config.pop(\"val_acc\")\n",
    "    num+=1\n",
    "# winsound.Beep(440,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(440,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandbWrite(project_name,best_config_array,best_history_array,name_runs):\n",
    "    for model_history,config in zip(best_history_array,best_config_array):\n",
    "        \n",
    "        \n",
    "\n",
    "        run = wandb.init(project=project_name, config=config)\n",
    "        name=name_runs+str(config[\"num_neurons\"])\n",
    "        run.name=name\n",
    "        # For not repeat two times the val_acc value in the config paper\n",
    "        \n",
    "        config = wandb.config\n",
    "        for epoch in range (len(model_history[0])):\n",
    "            wandb.log({'epochs': epoch,\n",
    "                'loss': round( model_history[0][epoch].item(),3),\n",
    "                'acc': round(model_history[1][epoch].item(),3), \n",
    "                'val_loss': round(model_history[2][epoch].item(),3),\n",
    "                'val_acc':round(model_history[3][epoch].item(),3)\n",
    "                }\n",
    "                )\n",
    "        \n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "\n",
    "\n",
    "project_name=\"T\"+stats[0:3].capitalize()+years[2:].replace(\"-\",\"\")+\"runs\"\n",
    "wandbWrite(project_name,best_config_array,best_history_array,\"onehidden_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Best model {'learning_rate': 0.001, 'num_neurons': 10, 'acti_fun': 'relu', 'loss': 0.0193, 'acc': 0.9987, 'val_loss': 1.4627, 'val_acc': 0.6}\n",
    "Best model {'learning_rate': 0.0001, 'num_neurons': 30, 'acti_fun': 'sigmoid', 'loss': 0.4884, 'acc': 0.9517, 'val_loss': 0.6466, 'val_acc': 0.64}\n",
    "Best model {'learning_rate': 0.0001, 'num_neurons': 50, 'acti_fun': 'sigmoid', 'loss': 0.5163, 'acc': 0.9437, 'val_loss': 0.656, 'val_acc': 0.66}\n",
    "Best model {'learning_rate': 0.0001, 'num_neurons': 100, 'acti_fun': 'relu', 'loss': 0.0628, 'acc': 0.9951, 'val_loss': 0.775, 'val_acc': 0.64}\n",
    "Best model {'learning_rate': 0.001, 'num_neurons': 150, 'acti_fun': 'relu', 'loss': 0.0135, 'acc': 0.9998, 'val_loss': 1.4478, 'val_acc': 0.64}\n",
    "Best model {'learning_rate': 0.0001, 'num_neurons': 200, 'acti_fun': 'relu', 'loss': 0.0487, 'acc': 1.0, 'val_loss': 0.8261, 'val_acc': 0.64} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regolarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config_array=[]\n",
    "best_history_array=[]\n",
    "possible_pL2=[0.0001,0.001,0.01]\n",
    "for num in number_neurons:\n",
    "    best_config_tmp={'val_acc':0}\n",
    "    best_history_tmp=[]\n",
    "    for el in possible_learning_rate:\n",
    "        for act in activation:\n",
    "            for pl2 in possible_pL2:\n",
    "            # Initialize model\n",
    "                model = NN(input_size=input_size,size_hidden_level=num,act=act,learning_rate=el,pl2=pl2).to(device)\n",
    "\n",
    "                \n",
    "                \n",
    "                config={\n",
    "                    \"learning_rate\":el,\n",
    "                    'num_neurons':num,\n",
    "                    'acti_fun':act,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"epochs\": num_epochs,\n",
    "                    \"pl2\":pl2,\n",
    "                    \"architecture\": \"OneHiddenLayerL2\",\n",
    "                    \"dataset\":years+\"_\"+stats\n",
    "                }\n",
    "                \n",
    "                best_config_tmp,best_history_tmp=model.MyTrain(kf=kf,config=config,best_config_tmp=best_config_tmp,\n",
    "                best_history_tmp=best_history_tmp,train_LL=train_LL,test_LL=test_LL,device=device)\n",
    "            \n",
    "                    \n",
    "    print(\"Best model\",best_config_tmp)\n",
    "    best_config_array.append(best_config_tmp)\n",
    "    best_history_array.append(best_history_tmp)\n",
    "    \n",
    "# Doing the mean between the 10 fold\n",
    "for num,array in  enumerate(best_history_array):\n",
    "    tmp=[ torch.mean(array[0],axis=0),torch.mean(array[1],axis=0),torch.mean(array[2],axis=0),torch.mean(array[3],axis=0)]\n",
    "  \n",
    "    best_history_array[num]=tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(440,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "project_name=\"T\"+stats[0:3].capitalize()+years[2:].replace(\"-\",\"\")+\"runs\"\n",
    "wandbWrite(project_name,best_config_array,best_history_array,\"onehiddenL2_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config_array=[]\n",
    "best_history_array=[]\n",
    "possible_p=[0.5,0.6,0.65]\n",
    "for num in number_neurons:\n",
    "    best_config_tmp={'val_acc':0}\n",
    "    best_history_tmp=[]\n",
    "    for el in possible_learning_rate:\n",
    "        for act in activation:\n",
    "            for p in possible_p:\n",
    "            # Initialize model\n",
    "                model = NN(input_size=input_size,size_hidden_level=num,act=act,learning_rate=el,dropout=p).to(device)\n",
    "\n",
    "                \n",
    "                \n",
    "                config={\n",
    "                    \"learning_rate\":el,\n",
    "                    'num_neurons':num,\n",
    "                    'acti_fun':act,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"epochs\": num_epochs,\n",
    "                    \"drop_rate\":p,\n",
    "                    \"architecture\": \"OneHiddenLayerDropout\",\n",
    "                    \"dataset\":years+\"_\"+stats\n",
    "                }\n",
    "                \n",
    "                best_config_tmp,best_history_tmp=model.MyTrain(kf=kf,config=config,best_config_tmp=best_config_tmp,\n",
    "                best_history_tmp=best_history_tmp,train_LL=train_LL,test_LL=test_LL,device=device)\n",
    "            \n",
    "                \n",
    "                    \n",
    "    print(\"Best model\",best_config_tmp)\n",
    "    best_config_array.append(best_config_tmp)\n",
    "    best_history_array.append(best_history_tmp)\n",
    "    \n",
    "# Doing the mean between the 10 fold\n",
    "for num,array in  enumerate(best_history_array):\n",
    "    tmp=[ torch.mean(array[0],axis=0),torch.mean(array[1],axis=0),torch.mean(array[2],axis=0),torch.mean(array[3],axis=0)]\n",
    "  \n",
    "    best_history_array[num]=tmp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(440,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "project_name=\"T\"+stats[0:3].capitalize()+years[2:].replace(\"-\",\"\")+\"runs\"\n",
    "\n",
    "wandbWrite(project_name,best_config_array,best_history_array,\"onehiddenDrop_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model {'learning_rate': 0.01, 'num_neurons': 10, 'acti_fun': 'relu', 'batch_size': 150, 'epochs': 500, 'architecture': 'OneHiddenLayer', 'dataset': '2019-20_advance', 'val_acc': 0.6779999732971191}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number_neurons=[10,30,50,100,150,200]\n",
    "number_neurons=[10]\n",
    "possible_learning_rate=[0.0001,0.001,0.01]\n",
    "# possible_learning_rate=[0.0001]\n",
    "\n",
    "best_config_array=[]\n",
    "best_history_array=[]\n",
    "for num in number_neurons:\n",
    "    best_config_tmp={'val_acc':0}\n",
    "    best_history_tmp=[]\n",
    "    for el in possible_learning_rate:\n",
    "        for act in activation:\n",
    "                \n",
    "                # Initialize model\n",
    "                model = NN(input_size=input_size,size_hidden_level=num,act=act,learning_rate=el).to(device)\n",
    "\n",
    "                \n",
    "                \n",
    "                config={\n",
    "                    \"learning_rate\":el,\n",
    "                    'num_neurons':num,\n",
    "                    'acti_fun':act,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"epochs\": num_epochs,\n",
    "                    \"architecture\": \"OneHiddenLayer\",\n",
    "                    \"dataset\":years+\"_\"+stats\n",
    "                }\n",
    "                \n",
    "                best_config_tmp,best_history_tmp=model.MyTrain(kf=kf,config=config,best_config_tmp=best_config_tmp,\n",
    "                best_history_tmp=best_history_tmp,train_LL=train_LL,test_LL=test_LL,device=device)\n",
    "               \n",
    "                \n",
    "                    \n",
    "    print(\"Best model\",best_config_tmp)\n",
    "    best_config_array.append(best_config_tmp)\n",
    "    best_history_array.append(best_history_tmp)\n",
    "    \n",
    "# Doing the mean and the variance between the 10 fold\n",
    "# (for the variance just the last element of each fold)\n",
    "# (mean for all the vectors of folds) \n",
    "num=0\n",
    "for array,config in  zip(best_history_array,best_config_array):\n",
    "    tmp1=[ torch.mean(el,axis=0) for el in array]\n",
    "    tmp2=[ torch.var(el[:,-1],axis=0) for el in array]\n",
    "    best_history_array[num]=tmp1\n",
    "    config[\"variance_loss\"]=tmp2[0].item()\n",
    "    config[\"variance_acc\"]=tmp2[1].item()\n",
    "    config[\"variance_val_loss\"]=tmp2[2].item()\n",
    "    config[\"variance_val_acc\"]=tmp2[3].item()\n",
    "    num+=1\n",
    "\n",
    "# winsound.Beep(440,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'num_neurons': 10,\n",
       " 'acti_fun': 'relu',\n",
       " 'batch_size': 150,\n",
       " 'epochs': 500,\n",
       " 'architecture': 'OneHiddenLayer',\n",
       " 'dataset': '2019-20_advance',\n",
       " 'val_acc': 0.6779999732971191,\n",
       " 'variance_loss': tensor(0.0115, device='cuda:0'),\n",
       " 'variance_acc': tensor(0.0001, device='cuda:0'),\n",
       " 'variance_val_loss': tensor(0.0956, device='cuda:0'),\n",
       " 'variance_val_acc': tensor(0.0007, device='cuda:0')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandbWrite(project_name,best_config_array,best_history_array,name_runs):\n",
    "    for model_history,config in zip(best_history_array,best_config_array):\n",
    "        \n",
    "        \n",
    "        # For not repeat two times the val_acc value in the config paper\n",
    "        for epoch in range (len(model_history[0])):\n",
    "            wandb.log({'epochs': epoch,\n",
    "                'loss': round( model_history[0][epoch].item(),3),\n",
    "                'acc': round(model_history[1][epoch].item(),3), \n",
    "                'val_loss': round(model_history[2][epoch].item(),3),\n",
    "                'val_acc':round(model_history[3][epoch].item(),3)\n",
    "                }\n",
    "                )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'num_neurons': 10, 'acti_fun': 'relu', 'batch_size': 150, 'epochs': 500, 'architecture': 'OneHiddenLayer', 'dataset': '2019-20_advance'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wandbWrite(\u001b[39m\"\u001b[39;49m\u001b[39mMa che \u001b[39;49m\u001b[39m\"\u001b[39;49m,best_config_array,best_history_array,\u001b[39m\"\u001b[39;49m\u001b[39monehiddenDrop_\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mwandbWrite\u001b[1;34m(project_name, best_config_array, best_history_array, name_runs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(config)\n\u001b[0;32m      6\u001b[0m \u001b[39m# For not repeat two times the val_acc value in the config paper\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m config\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mval_acc\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(config)\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (\u001b[39mlen\u001b[39m(model_history[\u001b[39m0\u001b[39m])):\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "wandbWrite(\"Ma che \",best_config_array,best_history_array,\"onehiddenDrop_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BasketTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2729838b41b4a5f7064d246ed1959d52d3f97d6ea7c6cf8ae385403751659321"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
